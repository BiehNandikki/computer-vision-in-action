<p align="left">
  <a href="https://github.com/Charmve"><img src="https://img.shields.io/badge/GitHub-@Charmve-000000.svg?logo=GitHub" alt="GitHub" target="_blank"></a>
  <a href="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy9aTmRoV05pYjNJUkIzZk5ldWVGZEQ4YnZ4cXlzbXRtRktUTGdFSXZOMUdnTHhDNXV0Y1VBZVJ0T0lJa0hTZTVnVGowamVtZUVOQTJJMHhiU0xjQ3VrVVEvNjQw?x-oss-process=image/format,png" target="_blank" ><img src="https://img.shields.io/badge/公众号-@迈微AI研习社-000000.svg?style=flat-square&amp;logo=WeChat" alt="微信公众号"/></a>
  <a href="https://www.zhihu.com/people/MaiweiE-com" target="_blank" ><img src="https://img.shields.io/badge/%E7%9F%A5%E4%B9%8E-@Charmve-000000.svg?style=flat-square&amp;logo=Zhihu" alt="知乎"/></a>
  <a href="https://space.bilibili.com/62079686" target="_blank"><img src="https://img.shields.io/badge/B站-@Charmve-000000.svg?style=flat-square&amp;logo=Bilibili" alt="B站"/></a>
  <a href="https://blog.csdn.net/Charmve" target="_blank"><img src="https://img.shields.io/badge/CSDN-@Charmve-000000.svg?style=flat-square&amp;logo=CSDN" alt="CSDN"/></a>
</p>

<b>第 23 章 大安全时代下的计算机视觉</b>

作者: 张伟 (Charmve)

日期: 2021/08/21

- 第 23 章 大安全时代下的计算机视觉
    - 23.1 什么是大安全
    - 23.2 人工智能 ✖️ 信息安全
      - 23.2.1 训练阶段的安全
      - 23.2.2 使用阶段的安全
        - 23.2.2.1 算法本身的安全
        - 23.2.2.2 训练框架安全
        - 23.2.2.3 业务流程安全
    - 23.3 计算机视觉模型攻击总述
    - 23.4 模型完整性
      - 23.4.1 逃逸攻击
      - 23.4.2 数据中毒攻击
    - 23.5 模型机密性
      - 23.5.1 模型萃取攻击
      - 23.5.2 成员推理攻击
      - 23.5.3 模型逆向攻击
      - 23.5.4 联邦学习下的模型安全
    - 23.6 模型可用性
    - 小练习
    - 项目实战22 - 神经网络模型嵌入盲水印
    - 项目实战23 - 利用深度学习在图像中嵌入盲水印
    - 小结
    - [参考文献]()

--- 


# 第 23 章 大安全时代下的计算机视觉

## 23.1 什么是大安全

## 23.2 人工智能 ✖️ 信息安全

深度学习因其特殊的产生环境，谈到信息安全方面，一切皆可成为攻击者切入的入口。因此，了解深度学习的全生命周期的安全成为学习研究和实现安全部署生产的必要条件。在本文中，主要从两个纬度进行阐述，训练阶段和使用阶段中数据安全和算法模型安全。

### 23.2.1 训练阶段的安全

数据来源 -> 数据使用 -> 数据保存

### 23.2.2 使用阶段的安全

#### 23.2.2.1 算法本身的安全

#### 23.2.2.2 训练框架安全

#### 23.2.2.3 业务流程安全

这一部分参考本开源项目 L0CV 框架，点击[这里](https://github.com/Charmve/computer-vision-in-action/tree/main/L0CV-Universe)查看。

## 23.3 计算机视觉模型攻击总述

```
                          |- 逃逸攻击 -> 模型（对抗样本）             ---- 模型训练阶段
  |- 模型完整性 - 对抗攻击 - |- 数据中毒攻击 -> 数据（后门攻击）
  |
  |             | 训练数据窃取 -> 数据  （成员推理、模型逆向 联邦学习）   ---- 模型使用阶段
  |- 模型机密性 - | 模型萃取 -> 模型（剪枝、知识蒸馏、量化与微调）
  |
  |- 模型可用性 - 模型代码的漏洞挖掘和利用                              ---- 模型使用阶段
  ```

**AI的完整性**主要是指模型学习和预测的过程完整不受干扰，输出结果符合模型的正常表现。这一块面临的攻击主要是对抗攻击，而对抗攻击又分为逃逸攻击和数据中毒攻击。其中逃逸攻击主要是通过生成对抗样本的方式来逃出模型的预测结果，数据中毒攻击主要是从数据层面对模型进行干扰。

**AI的机密性**主要是指模型需要确保其参数和数据（无论是训练数据还是上线后的用户数据）不能被攻击者窃取。这一块面临的攻击主要是模型萃取和训练数据窃取攻击。AI的机密性非常重要，一是因为数据往往是一个公司的安身立命之本，如果被攻击者窃取，对公司和用户都是致命打击。二是因为模型的训练成本非常高昂，一个好的模型背后不单单是一个好的算法团队，还有长达几个月甚至几年的迭代时间成本，一旦被竞争对手窃取，后果也将不堪设想。

**AI的可用性**主要是指模型能够被正常使用。这一块面临的攻击主要是传统的一些软件漏洞，如溢出攻击和DDos攻击。

接下来，本章节将依次介绍针对AI 完整性、可用性和机密性的攻击。


## 23.4 模型完整性

AI模型的完整性主要体现在模型的学习和预测过程完整不受干扰，输出结果符合模型的正常表现上。这是研究人员能相信AI模型的输出结果的根本，也是AI模型最容易受到攻击的地方。

针对AI模型完整性发起的攻击通常称为“对抗攻击”。 对抗攻击通常分为两类，一类是从模型入手的逃逸攻击，一类是从数据入手的数据中毒攻击。

### 23.4.1 逃逸攻击

逃逸攻击(Evasion Attacks)是指攻击者在不改变目标机器学习系统的情况下，通过构造特定输入样本以完成欺骗目标系统的攻击。

image-20190709232329927.png

如上图所示[1]，攻击者可以为左边的熊猫图片增加少量干扰，让google的图片识别系统将其判断为长臂猿。

这其中的根本原因，在于模型没有学到完美的判别规则。虽然图片识别系统一直试图在计算机上模仿人类视觉功能，但由于人类视觉机理过于复杂，两个系统在判别物体时依赖的规则存在一定差异。比如说我们可能是通过熊猫的黑眼圈，黑耳朵，黑手臂还有熊脸判断出它是一个熊猫，但图片识别系统可能只是根据它躺在树枝上的这个动作，就将其判断为了长臂猿。因此完美的判别规则和模型实际学到的判别规则之间的差距，就给了攻击者逃脱模型检测的可趁之机。

试想，如果模型真的和人所使用的判别规则一模一样，比如一样是通过黑眼圈，黑耳朵，黑手臂还有熊脸判断出它是一个熊猫，那么这个模型就不存在被逃逸攻击的可能性了。因为一旦模型判断失败，那么人类同样会判断失败。

逃逸攻击的应用场景
逃逸攻击目前已经受到了广泛的关注，并被应用到了大量场景上，如攻击自动驾驶汽车、物联网设备、语音识别系统等，可以说“哪里有AI，哪里就有逃逸攻击”。下面是三个例子的详细介绍。

让自动驾驶汽车错误识别路边标示[2]
Xnip2019-07-12_00-29-32.jpg

在原停止标志图像（左边）中，停止标志可以被成功地检测到。在中间图像，在整个图像中添加了小的干扰，则停止标志不能被检测到。在最后一个图像中，在停止标志的符号区域添加小的干扰，而不是在整个图像，停止标志被检测成了一个花瓶。

让人脸识别系统错误识别人脸[3]
Xnip2019-07-12_00-31-43.jpg

来自CMU的一篇论文，通过给人们佩戴专门设计过的眼镜架，便可以骗过最先进的面部识别软件。一副眼镜，不单可以让佩戴者消失在人工智能识别系统之中，而且还能让AI把佩戴者误以为是别人。考虑到人脸识别系统现如今应用范围之广，一旦该手段被恶意势力所利用，后果必然不堪设想。

攻击语音识别系统[4]
Xnip2019-07-09_23-48-36.jpg

从正常命令倒推产生混淆的音频命令（如一段人类无法辨认的噪音），从而在三星Galaxy S4以及iPhone 6上面被正确识别为相对应的语音命令，变为让手机切换飞行模式、拨打911等。

逃逸攻击的攻击模式
逃逸攻击有多种分类方式，最常见的分类方式是根据其是否了解模型和是否有攻击目标来分。

根据攻击者是否了解模型，可以分为黑白盒攻击两类。

白盒攻击：攻击者能够获知机器学习所使用的算法，以及算法所使用的参数。攻击者在产生对抗性攻击数据的过程中能够与机器学习的系统有所交互。
黑盒攻击：攻击者并不知道机器学习所使用的算法和参数，但攻击者仍能与机器学习的系统有所交互，比如可以通过传入任意输入观察输出，判断输出。
根据是否有攻击目标，可以分为无目标攻击和有目标攻击两类。

无目标攻击（untargeted attack）：对于一张图片，生成一个对抗样本，使得标注系统在其上的标注与原标注无关。换而言之，只要攻击成功就好，对抗样本的最终属于哪一类不做限制。
有目标攻击（targeted attack）：对于一张图片和一个目标标注句子，生成一个对抗样本，使得标注系统在其上的标注与目标标注完全一致。换而言之，不仅要求攻击成功，还要求生成的对抗样本属于特定的类。
比如之前的熊猫例子，如果我只需要模型无法识别出这是一只熊猫，系统把它识别成猩猩兔子都可以，就叫做无目标攻击。如果我不仅需要系统将其识别为一只熊猫，还需要系统将它识别为我指定的一个东西，比如说我想让他将熊猫识别为长臂猿，这就叫做有目标攻击。

逃逸攻击的研究方向
在逃逸攻击领域，目前主要有两类研究方向：

设计更具攻击性的对抗样本去作为神经网络鲁棒性的评估标准（如FGSD、IFGSD、Deepfool、C&W等）
研究针对对抗样本攻击的防御方法，提升NN模型的鲁棒性（如对抗训练、梯度掩码、随机化、去噪等）
下面我们依次来介绍这两类研究方向的相关算法。

常见对抗样本生成方式
基于梯度的攻击
基于梯度的攻击是最常见也是最容易成功的一种攻击方法。它的核心思想可以用一句话来概括：以输入图像为起点，在损失函数的梯度方向上修改图像。

执行此类攻击主要有两种方法：

一次攻击(One-shot Attacks)：攻击者在梯度方向上迈出一步，如FGSM、 T-FGSM
迭代攻击(Iterative attacks)：攻击者在梯度方向迈出多步，逐渐调整，如I-FGSM
FGSD（Fast gradient sign method）[5]
FGSD是一种基于梯度生成对抗样本的算法，其训练目标是最大化损失函数 J(x∗,y) 以获取对抗样本x∗，其中 J 是分类算法中衡量分类误差的损失函数，通常取交叉熵损失。最大化 J 即时添加噪声后的样本不再属于 y 类，由此则达到了上图所示的目的。在整个优化过程中，需满足 L∞ 约束 ‖x∗−x‖∞≤ϵ，即原始样本与对抗样本的误差要在一定范围之内。

x∗=x+ϵ⋅sign(▽xJ(x,y))

其中sign()是符号函数，括号里面是损失函数对x的偏导， x是输入的图片（原始图片），x∗ 是生成的对抗样本，J是分类器的损失函数，y是输入的图片x的标签。

比如说我们最常见的这个熊猫图，这个公式就可以被形象得翻译成如下图所示。

v2-04ba4362fb73f51a087ae647f501f6ad_r.jpg

Targeted fast gradient sign method (T-FGSM)

与FGSM类似，T-FGSM还是在计算梯度，但这个算法的不同之处在于，它梯度下降的方向是朝着目标对象的，也就是说，在梯度下降的过程中，我们在努力朝自己期望的标签的方向前进。公式如下：

x∗=x−ϵ⋅sign(▽xJ(x,ytarget))
这里的ytarget是目标对象的label

I-FGSD(Iterative gradient sign Method)[6]

前面的FGSD和T- FGSD都是一次攻击（One-shot Attacks），而I-FGSD则是迭代攻击（Iterative Attacks）的一个例子，在这个算法里，攻击者以一个小的脚步 α 多次应用快速迭代法：

x∗0=x,xt+1=x∗t+α⋅sign(▽xJ(x∗t,y))
其中为了使得到的对抗样本满足 L∞ （ 或 L2 ）约束，通常将迭代步长设置为 α=ϵ/T ，T为迭代次数。

下图展示了IFGSD以不同的ϵ生成的对抗样本，未添加噪声前，样本被判定为53.98%的可能性为一个洗衣机，而添加噪声后，判定为洗衣机的概率变小，分类模型将其分类为保险箱，甚至音响。

image-20190710221422409.png

实验表明，I-FGSD在白盒攻击上，效果比FGSM和T-FGSM好。但在黑盒攻击上，one-shot攻击的效果比迭代攻击的效果好，部分原因是迭代的方式容易过拟合。

超平面分类思想
除了基于梯度的对抗样本生成思想以外，我们还可以基于超平面分类的思想来生成对抗样本.

Deepfool[9]

Deepfool是基于超平面分类思想的一种对抗样本生成方法。了解过SVM的人，应该都对超平面分类思想不陌生。事实上，在二分类问题中超平面是实现分类的基础。那么要改变某个样本的分类，最小的扰动就是将x挪到超平面上，让这个距离的代价最小，就是最小的扰动。这里为了便于描述，我们拿线性二分类问题入手，多分类问题也是类似解法，就不赘述了。

如下图所示，假设有原始样本x，标签为y，我们期望通过对原始样本添加人类不容易发觉的噪音 r ，生成对抗样本 x∗ ，使分类器所生成的对抗样本的标签判定为 y∗，其中 y≠y∗，这里的目标是找到最小的r。 F 为该问题的分类面， F=x:f(x)=0 ,其中 f 是一个线性二分类器f(x)=ωTx+b 。

image-20190711003515605.png

要使 x0被分为另外一类，最简单的扰动即方向垂直于分类面，大小为 △(x0,f) 的扰动。

r∗(x0)=argmin||r||2   s.t.sign(f(x0+r))≠sign(f(x0))
r∗(x0)=−f(x0)||ω||22ω

根据上式，可以很容易地计算得到 r∗ ,但是这个扰动值只能使样本达到分类面，而不足以越过，故最终的扰动值为 r∗(1+η)，η≪1 ,实验中一般取0.02。

常见防御方式
介绍了如何生成对抗样本，接下来我们来看一下从防御层面我们能做什么。如下图所示，主要有三个思考角度：改变输入，改变网络和添加外部防御。

Picture1.jpg

常见防御方法：

对抗训练：训练集除了真实数据集外，还有加了扰动的数据集
梯度掩码：隐藏原始梯度
随机化：向原始模型引入随机层或者随机变量。使模型具有一定随机性，全面提高模型的鲁棒性，使其对噪声的容忍度变高。
去噪：数据输入模型前，先对其进行去噪
对抗训练[10]
对抗训练防御的主要思想是：在模型训练过程中，将对抗样本一并加入到训练样本中，组成新的对抗样本，如此随着训练次数增多，模型的准确率会增加，模型的鲁棒性也会增加。

比如说，我们要训练一个模型来识别标签 y∈−1,1，输入输出关系为 P(y=1)=δ(ωTx+b), δ是sigmoid函数 。根据下式作梯度下降：

image-20190711012430173.png

注意这里−sign(ω)是梯度，将上式中的x换成加了扰动的x，就可以得到右边的式子。这里看起来很像L1正则化，但不同的是。这里的惩罚项是加在激活函数里，而不是加到损失函数里。这也意味着，随着激活函数趋于饱和，模型有足够的信心进行预测时，惩罚项会逐渐消失。增加惩罚项也会有不好的影响，可能会使欠拟合更加恶化。

梯度掩码/隐藏（Gradient Masking）[11]
前面我们说过，大部分“对抗样本”都是基于模型梯度来构建的。比如说对于一个图像识别模型，当我们给了它一张熊猫的图，它最终将其分类为熊猫时，实际上是在说：“这个图片在熊猫这个类别上的置信度最高”。换而言之，这张图片其实在“长臂猿”等其他类别也是有着置信度的，只是这个置信度很小而而已。因此攻击者只需要反复测试超平面的哪一个方向会导致长臂猿的置信度增加，然后朝这个方向“推一把”，添加一些扰动，那么这张经过修改的新图片就会被错误识别为“长臂猿”了。

我们从防御的角度来思考，如果我们的模型只输出熊猫，不输出其他信息，不透露给攻击者有用的信息，那么攻击者便无法得知应该在图片哪个方向上“推一把”，就能够阻挠攻击者发起的对抗攻击了。

但实际上，这是一个“治标不治本”的办法，因为模型并没有变得更稳定，我们只是提高了攻击者弄清楚模型防御弱点的难度。

此外， 攻击者也可以自己训练一个拥有梯度的光滑模型来制作“对抗样本”，然后将这些“对抗样本”放进我们保护的非平滑模型里，从而实现对模型的破解。

防御性蒸馏[12]
防御性蒸馏是Papernot等人训练深度神经网络的蒸馏法提出来的一个防御方法。

首先简单的介绍一下蒸馏法的原理。在机器学习领域中有一种最为简单的提升模型效果的方式：在同一训练集上训练多个不同的模型，然后在预测的时候使用平均值来作为预测值（集成学习）。但是这种组合模型需要大量计算资源，特别是当单个模型都非常复杂的时候。

有相关的研究表明，复杂模型或者组合模型中的“知识”可以通过合适的方式迁移到一个相对简单模型之中，进而方便模型推广（迁移学习）。这里所说的“知识”就是模型参数，简单说就是，我们训练模型是想要学到一个输入向量到输出向量的映射，而不必太过于关心中间的映射过程。

那么防御性蒸馏也是同理，我们希望将训练好的复杂模型推广能力“知识”迁移到一个结构更为简单的网络中，或者通过简单的网络去学习复杂模型中“知识”，从而避免攻击者直接接触到我们的模型。

防御的具体思路是：

首先根据原始训练样本 X 和标签 Y 训练一个初始的深度神经网络，得到概率分布 F(X)。
然后利用样本 X 和第一步的输出结果F(X) （作为新的标签）训练一个架构相同、蒸馏温度 T 也相同的蒸馏网络，得到新的概率分布Fd(X) ，再利用整个网络来进行分类或预测，这样就可以有效的防御对抗样例的攻击。流程如下图所示：
image-20190711221635366.png
效果：

使得通过梯度生成的对抗样本的攻击性大大降低，并且对原始任务的准确性没有很大的影响
蒸馏温度越高，网络的平均梯度越小，生成具有足够攻击性的对抗样本越困难，间接地提升了模型的鲁棒性
本质上没有解决模型对于对抗样本鲁棒性差的问题。因此，其只能有效的抵抗白盒攻击，对黑盒攻击则无能为力
仅适用于基于概率分布的DNN模型，不适用于建立通用的强鲁棒性的DNN模型
随机化Randomization[13]
前面我们介绍过，对抗攻击从生成过程来看可以分为单步攻击（只执行一步梯度计算）和迭代攻击（进行多次迭代进行攻击）。

从效果来看，迭代攻击生成的对抗样本容易过拟合到特定的网络参数，迁移性较弱。而另一方面，单步攻击（如FGSM）具有更好的迁移能力，但是攻击性较差，可能不足以欺骗网络。正是因为迭代攻击的泛化能力不强，所以我们可以考虑低级图像变换（如调整大小、填充、压缩等）的方法来破环迭代攻击生成的对抗扰动的特定结构，从而起到很好的防御效果。如果这里的变换手段是随机变换，那么攻击者就无法确认变换方式，所以甚至可能抵御白盒攻击。此外，这种随机变换的方式与对抗训练的方式完全不同，我们可以考虑把两个防御方法结合起来使用，将有希望有效防御单步和迭代攻击，包括白盒和黑盒攻击。

此外，因为这个防御方法只是对输入样本进行了一个随机转换，没有网络参数，所以不需要重新训练或微调模型，提高了这个方法的实用性。

如下图所示，论文里主要添加了两个随机层。
image-20190711231512951.png

第一个随机层是随机调整大小层。假设原始图像Xn的大小为 W×H×3，我们可以通过随机调整大小层 将Xn调整为一个新的图像X′n ，大小为 W′×H′×3 。注意，这里的 |W′−W|和 |H′−H|都要控制在一个合理的小范围内，否则干净图像的分类性能会显著下降。

第二个随机层是随机填充层，以随机的方式在调整大小后的图像周围填充零像素点。具体地，就是将 W′×H′×3 填充为 W″×H″×3 ，而填充后的图像尺寸，即为分类器的输入尺寸，所以填充后的大小是固定的 。对照着上图来看填充过程的话，如果在左边填充ω 列零，那么就要在右边填充 W″−W−ω 列零，上下同理。这样的话，一共有 (W″−W′+1)×(H″−H′+1) 种填充方式。

将原始图像经过上述两个随机层转换后，再传递给CNN进行分类。

在论文里，作者假设了三种攻击场景（香草攻击、单模式攻击、集合模式攻击）来测试这个方法的防御性能。

香草攻击(vanillaattack): 攻击者不知道随机层的存在，目标模型就是原始网络。
单模式攻击：攻击者知道随机层的存在。为了模仿防御模型的结构，选择目标模型作为原始网络+随机化层，只有一个预定义模式。
集合模式攻击：攻击者知道随机化层的存在。为了以更具代表性的方式模仿防御模型的结构，目标模型被选择为具有预定义模式集合的原始网络+随机化层。
效果：

三种攻击场景（香草攻击、单模式攻击、集合模式攻击）下，防御模型性能普遍优于目标模型
当攻击者不知道随机层存在时，随机层防御的效果最佳
当攻击者知道有随机层的存在，但由于随机模式众多，不可能考虑所有的随机模式，所以防御依然有效果
去噪[14]
第四种方法是去噪，思想非常简单，既然攻击者是通过增加噪音来生成的对抗样本，那么我们只要把样本中的噪声去掉，尽可能将其恢复成原始样本不就好了吗？

这篇文章里，作者尝试使用了两个不同架构的神经网络作为去噪器来进行去噪。一个是 Denoising Autoencoder（DAE），另一个是 Denoising Additive U-Net（DUNET），两者结构如下图所示。其中x表示原始图片，x̂ 表示去噪后的图片。作者的去噪器(PGD)的损失函数为L=|x−x̂ | ，表示原始图片和去噪后的图片的差异，最小化损失函数就可以得到尽可能接近原始图片的去噪后图片。

image-20190711232718379.png

但是这种去噪器在实验过程中，出现了一个现象，就是输入经过去噪以后，正确率反而有点下降。这主要是因为去噪器不可能完全消除扰动，剩下的微小扰动在预训练好的卷积网络模型中逐层放大，对卷积网络的高维特征产生了较大的扰动，最终导致网络分类错误。

针对这个问题，作者又提出了一种以高级表示为导向的去噪器（HGD），结构如下图所示：

image-20190711233318728.png

从图中可以看出。HGD与PGD最大的区别在于不再以去噪后图片与原始图片的差异 L=|x−x̂ |作为损失函数，而是将去噪后的图片和原始图片都输入到预训练好的深度神经网络模型中，将最后几层的高级特征的差异 L=|f1(x̂ )−f1(x)|作为损失函数来训练去噪器，这样就避免了PGD的扰动逐层放大的问题。

效果：

两种网络结构在处理干净图片时准确率都有所下降。
PGD的差异随着网络逐层放大，最后快接近于未经过去噪的对抗样本，HGD的误差较小，变化不是很大。
HGD能在特征层面有效的抑制扰动，在黑白盒攻击面前都有很强的鲁棒性。
但同时这个方法也存在着一定的缺陷：

依赖于微小变化的可测量，问题没有完全解决
除非对手不知道HGD的存在，否则还是可能受到白盒攻击

### 23.4.2 数据中毒攻击

#### 23.4.2.1 数据投毒攻击是什么
数据投毒/投毒攻击（Data Poisoning Attack）是什么？这里我们用几个例子来说明。


对于一辆无人驾驶汽车，不同于逃逸攻击直接构造对抗样本对模型和系统发起攻击，由于智能汽车的识别模型通常是基于历史数据进行训练定期生成的，那么黑客就可以产生一些实时的数据或者说一些黑客以前已经习惯于走这些道路的数据，把这些数据输入到云端或者智能汽车系统中，从而实现让智能汽车按照黑客想要的路径去前进，利用数据投毒的方式来攻击人工智能系统。

此外，当前市面上的问答式机器人/智能交互AI，如微软小冰、QQ小冰等，它们通过庞大的语料库来学习，还会将用户和它的对话数据收纳进自己的语料库里，因此我们也可以在和它们对话时进行“调教”，从而实现让其说脏话甚至发表敏感言论的目的。


在业务安全领域，阿里集团安全资深总监路全在CCF大会上也分享过一个数据中毒的例子 :

“阿里的有些端口每天会来很多爬虫。有些爬虫即使被杀掉了，还是不断的来。这就非常奇怪了，因为他是在浪费他的钱。
后来他们发现，这些爬虫其实也是在用一种非常聪明的方式污染模型，因为他除了有大量低级爬虫或者说低级流量外，他还有一部分高级的爬虫。他不怕低级的被模型识别和杀掉，但是他知道模型会被大量低级爬虫样本的特征所带偏，所以他实际上是用低级爬虫做诱饵，然后让高级爬虫帮他达到目的。”

#### 23.4.2.2 数据中毒的根本原因

通过前面的例子，细心的小伙伴可能也发现了，数据中毒的根本原因，其实是传统机器学习方法并没有假设输入模型的数据可能有误，甚至有人会故意搅乱数据的分布。

通常情况下，如果我们通过时间滑窗的方式来生成模型，我们期望的是今天的数据和上周数据的平均分布是差不多的，或者说每周一的数据分布情况都差不多。基于这个假设，模型才成立。而这个假设恰好就让攻击者有机可乘了。


#### 23.4.2.3 数据投毒攻击分类和解决方案
在这一部分，我们详细介绍数据投毒攻击的分类和对应的解决方案。

##### 模型偏斜(Model skewing)

模型偏斜是指攻击者试图污染训练数据，来让模型对好坏输入的分类发生偏移，从而降低模型的准确率。


例如，Google反滥用研究团队的主管曾经介绍过发生在Google垃圾邮件分类器上的数据投毒攻击案例。一些最高级的垃圾邮件制造者试图通过将大量垃圾邮件提交为非垃圾邮件来使 Gmail 邮件分类器发生偏斜。

如图所示，2017 年 11 月底至 2018 年初，Gmail遭遇了至少 4 次大规模恶意攻击试图让分类器偏斜。




因此，在设计 AI 防御机制时，需要考虑以下事实:

攻击者一定会非常积极地将正常数据和异常数据之间的那条分界线转移到对他们有利的位置。
这种攻击的解决方案，通常有三种：

使用合理的数据采样。 需要确保一小部分实体（包括 IP 或用户）不能占模型训练数据的大部分。特别是要注意不要过分重视那些伪装成正常样本的恶意样本。这可能通过限制每个用户可以贡献的示例数量，或者基于报告的示例数量使用衰减权重来实现。
将新训练的分类器与前一个分类器进行比较以估计发生了多大变化。例如，可以执行 dark launch，并在相同流量上比较两个模型的输出结果。还可以对一小部分流量进行 A/B 测试和回溯测试。
构建标准数据集，分类器必须准确预测才能投入生产。 在标准数据集里包含一组精心策划的攻击和系统的正常数据。只有当模型在这个标准数据集上的效果达标的情况下，才能上线该模型。从而避免数据投毒攻击直接对生产环境上的模型造成负面影响。
##### 反馈武器化(Feedback weaponization)：
反馈武器化是指将用户反馈系统武器化，来攻击合法用户和内容。一旦攻击者意识到模型利用了用户反馈对模型进行惩罚(penalization)，他们就会基于此为自己谋利。




比如说如果我们想攻击一个交互系统，比如说智能客服，每次它回答问题后，下面会弹出一个服务评分选项。那只要我们持续地给正确答案打1星，错误答案打5星，如果背后的模型没有对数据投毒攻击有所防范，那这个智能客服模型就会被成功干扰。

因此，在构建系统时，需要考虑以下假设:

任何反馈机制都将被武器化以攻击合法用户和内容。
解决方案：

不要在反馈和惩罚之间建立直接循环。相反，在做出决定之前，确保评估反馈真实性，并将其与其他特征结合起来。

##### 后门攻击（backdoor attacks）

在一部分文献里，后门攻击被列入了逃逸攻击或数据投毒攻击，在另一部分文献里，后门攻击被单独归为一类。在本文中，我们将后门攻击视作模型投毒攻击的一个子类。

模型后门(backdoor)是指通过训练得到的、深度神经网络中的隐藏模式。和逃逸攻击中直接让模型将一个物品识别错误不同，后门攻击是指当且仅当输人为触发样本(trigger)时，模型才会产生特定的隐藏行为;否则,模型工作表现保持正常。

针对机器学习中极其常用的MNIST手写数据集，有学者提出了通过数据投毒方法来注入后门数据集，并达到99%以上的攻击成功率，但不会影响模型在正常手写样本上的识别性能。


如上图所示，图左是一个正常的模型，图中是黑客期望的模型的样子（导致结果识别为8，而非7）。但黑客没有办法修改模型，所以他必须将后门合并进用户指定的网络架构里（右）

下面这张图 ，最左边就是原始的手写图片，中间和右边的图片分别被添加了后门（右下角的白点）。


对细节感兴趣的可以去读读这篇论文《BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain》[]。




## 23.5 模型机密性

如今，机器学习已演化为了一种服务模式，即机器学习即服务(Machine learning as a service)。互联网公司通过模型调用接口来满足用户对各类机器学习模型的使用需求，如Google的多种预测API、亚马逊机器学习(AmazonML)、MicrosoftAzure机器学习 (Azure ML)等。在这些平台上，用户不清楚模型和训练算法的实现细节，仅能选择简单的模型，模型的结果也高度依赖于输入模型的数据质量和打标结果。机器学习服务提供商通过用户调用该API的费用来收费，因此对于用户而言，这些API接口都是黑盒。




MLaaS架构

针对这样的黑盒，攻击者有办法获取到模型的细节或是模型背后的数据吗？

答案是，有的，而且不止一种。

在本文中，我们就将介绍黑客破坏模型机密性的三种方法，分别是针对模型的模型萃取攻击（Model Extraction Attacks）和针对数据的成员推理攻击（Membership Inference Attacks）和模型逆向攻击（Model Inversion Attack）。


机器学习模型的机密性指：机器学习系统必须保证未授权用户无法接触到模型的相关信息，包括模型本身的信息（如模型参数、模型架构、训练方式等）和模型用到的训练数据。

### 23.5.1 模型萃取攻击

模型萃取攻击（Model Extraction Attacks），也称为模型提取攻击，是一种攻击者通过循环发送数据并查看对应的响应结果，来推测机器学习模型的参数或功能，从而复制出一个功能相似甚至完全相同的机器学习模型的攻击方法。

这种攻击方法由Tramèr等人在2016年提出，并发表于信息安全顶级会议Usenix上，并分别展示了针对函数映射类模型（LR、SVM、神经网络）、决策树模型和不输出置信度的模型的提取方式。

#### 23.5.1.1 针对函数映射类模型

对于逻辑回归（LR）、支持向量机(SVM)、神经网络（NN）这类算法而言，我们可以将其模型看做是一个函数f(x)，模型的输入是x，模型的输出结果是f(x)，模型在训练过程中通过优化函数f(x)里的参数来达到分类的目的。因此，如若我们想要对这类模型进行萃取，只需求解f(x)里的参数值就可以了，实际上就是一个解方程的过程。

如对于下图中的逻辑回归模型：

[公式]

函数的参数是 w 和 b，其中w是n维的权重向量，b 是偏置向量。我们对 sigmod 函数进行求反，得到一个线性函数 $w*x + b$，因为$w$ 是n维的权重向量，b是1维的偏置向量，因此总共有n+1个参数需要求解，也就是说，至少需要 $n+1$ 个等式就可以借出这个方程，所以从理论上而言，我们只需要查询 $n+1$ 次便可窃取到这个逻辑回归模型。


而对于多分类模型，只需要举一反三即可。

多分类模型要完成对 $c（c>2）$个类别进行分类，置信度则是输入在每个类别的概率分布，输出的置信度是 n 维向量。则其输出的置信度公式为




其未知参数有 $c*(n+1)$ 个（每个类别存在 $n+1$ 个未知数），且为非线性函数。换而言之，我们通过多次访问构建的方程组是非线性方程组，且每个方程都是超越方程。针对这种情况，我们可以利用梯度下降方法来实现对方程的求解：构造一个损失函数为凸函数，转化为凸优化问题求解，其中全局最优解为模型的参数。


#### 23.5.1.2 针对决策树

与回归模型相反，决策树不计算输入值属于某个分类的概率，而 是将输入分隔成多个离散区域，并给每个区域分配标签和置信度值。 决策树上每个叶子节点都有其对应的置信度值。假设每个叶子节点都有不同的置信度值，则可利用置信度作为决策树叶子节点的伪标识。同时，很多 MLaaS 提供商为 了提升 API 访问的可访问性，即使输入数据是部分特征依然可以得到输出结果。这就为针对决策树进行路径寻找攻击提供了必要条件。

简单来说，针对决策树模型的提取是自顶向下的，首先我们获取到决策树根节点的标识，之后依次设置不同的特征来到达不同的节点，递归搜索决策树的结构特征。



#### 23.5.1.3 针对不输出置信度的模型

为了避免遭受到模型萃取攻击，部分云厂商不再提供机器学习预测结果的置信度，而只给出“是”或“否”的答案。但论文作者表示，这类模型仍然可能遭受到模型萃取攻击。


如上图所示，首先，攻击者随机生成样本并输入目标模型，得到预测结果。然后将这些数据作为训练集，在本地训练一个和目标模型同类的本地模型，将靠近本地模型分类边界的数据点抽取出来，再对目标模型进行访问，获得新的训练集，并继续更新本地模型，直到本地模型和目标模型的误差足够小。

这样，本地模型就高度相似于目标模型了。

#### 23.5.1.4 模型水印

除了隐藏输出结果的置信度外，为了避免模型被盗用、保护知识产权，IBM的研究者提出了模型水印( watermarking)的概念。


如上图所示——

首先算法人员基于训练集，生成一系列带有水印的数据集，如上图中的automobile和airplane, 其中airplane是嵌有水印的图片
然后将训练集和带有水印的数据一起输入到神经网络中
如果竞争者盗用了他们的模型，那么当输入带水印的图片时，对方的模型就会将其识别为airplane，而不是automobile，以此证明对方剽窃了自己的模型
研究人员开发了三种算法来生成三种类型的水印：

将“有意义的内容”与算法的原始训练数据一起嵌入模型

- 嵌入不相关的数据样本
- 嵌入噪声

### 23.5.2 成员推理攻击
成员推理攻击（Membership Inference Attacks）是指给定数据记录和模型的黑盒访问权限，判断该记录是否在模型的训练数据集中。

这个攻击的成立主要是基于这样一个观察结果：对于一个机器学习模型而言，其对训练集和非训练集的uncertainty 有明显差别，所以可以训练一个Attack model 来猜某样本是否存在于训练集中。


如对于上图所示，首先，攻击者将一个样本输入到目标模型中，并获得相应的预测结果，然后将样本的标签、目标模型的预测结果输入到Attack Model里，判断这个记录是否在目标模型的训练集中。

由于Attack Model需要训练集才能生成，因此作者进一步提出了影子模型（shadow model）这个概念。


影子模型很好理解，生成方式和模型萃取攻击相似，即影子模型 T' 与目标模型 T 的架构要一样，然后随机生成data x，把 x 输入 T 得到的label 预测结果p当作 y，再用这些(x, y) 去训练 T'。 因为两个模型的架构相同， T 与 T’ 预测的机率分布也是相似的，所以针对影子模型 T’ 的 attack model 也可以对目标模型 T 做成员推理攻击。

影子模型训练数据的生成方式：

1. 基于模型的合成方式 Model-based synthesis：如果攻击者没有真正的训练数据,也没有关于其分布的任何统计，他可以利用目标模型本身为影子模型生成合成训练数据。直观来看，高置信度的目标模型分类的记录应当在统计上类似于目标的训练数据集，因此能用于训练影子模型。

2. 基于统计的合成方式 Statistics-based synthesis ：攻击者可能拥有目标模型的训练数据的一些统计信息。例如，攻击者可能事先知道不同特征的边缘分布。在实验中，可以通过独立地从每个特征的边缘分布中进行采样，来生成影子模型的训练集。

3. 含噪音的真实数据 Noisy real data ： 攻击者可以访问一些与目标模型的训练数据类似的数据，这些数据可以被视为“噪音”版本。
生成影子模型后，构造攻击模型的训练集（训练集的标签，和影子模型的输出结果），并对攻击模型进行训练就好了。


防御手段：

- 一些防止 overfitting 的方法可以用来防御成员推理攻击，如 droupout、regularization

- 对预测结果做一些后续处理也可以防御成员推理攻击，如取 top k classes、rounding、增加 entropy 等手法

### 23.5.3 模型逆向攻击

破坏模型机密性的第三种手段是模型逆向攻击（Model Inversion Attack）。这种攻击方法利用机器学习系统提供的一些API来获取模型的一些初步信息，并通过这些初步信息对模型进行逆向分析，获取模型内部的一些隐私数据。这种攻击和成员推理攻击的区别是，成员推理攻击是针对某一条单一的训练数据，而模型逆向攻击则是倾向于取得某个程度的统计信息。

例如，用模型和某位病患的人口属性，可以回推出该病患的遗传资料（即该模型的input）; 用一个名字（模型的output）回推出这个人的脸部影像，如上图所示，图右是训练集中的原图，图左是从模型逆向中推出的这个人的长相，攻击者只有这个人的名字和人脸识别模型的API调用权限。


这种攻击是怎么进行的呢？我们用最简单的线性回归模型来举例。


如上所示，假如我们有一个基于病人的人口统计信息、医疗记录和基因特征来预测病人应该服用多少剂量药物的线性回归模型。我们假定基因特征是敏感特征x1, 那么模型逆向攻击的目的就是通过构造大量的样本，来推断病人的基因特征x1的值是多少。

这里，模型的表达式为 [公式] 。首先，攻击者需要对病人的其他信息有一定了解，如人口统计信息和医疗记录，从而减少遍历次数，其次，攻击者需要知道x1的取值范围是多少。之后，只要通过不停构造测试数据，并将模型返回的结果和实际结果进行对比，就可以获得该患者的敏感特征x1了。

如果我们对数据源一无所知，只有一个标签，比如姓名，还可以发起攻击吗？

我们拿人脸识别系统来举例。

首先，因为我们对数据源一无所知，我们需要用随机生成的输入向量来冷启动攻击，然后通过在输入空间使用梯度下降来最大化模型在目标预测值上的置信度，从而重构出这个名字对应的人脸。

详细的过程比较复杂，感兴趣的同学可以自行参考这篇论文[7]。

### 23.5.4 联邦学习

#### 23.5.4.1 联邦学习概述

联邦学习（federated learning）的愿景是在不共享数据的情形下进行多方联合机器学习，本质上是一种数据访问受限的分布式机器学习框架。

相比于经典的分布式机器学习，<b>联邦学习的第一层受限是数据隔离——数据在各个终端不共享，不均衡，交互通信要尽量少。</b>微软和 CMU 合作的论文介绍了这方面的工作[8]。研究人员分析了在数据隔离的限制下一类分布式方差缩减算法（Variance Reduced Methods, i.e.,SVRG,SARAPH,MIG）的理论性能。在最自然的分布式设置下（终端节点运算内循环，参数服务器运算外循环）取得线性收敛性(强凸目标函数)，并且算法的时间复杂度对条件数的依赖取得目前理论上的最好结果。分布式方差缩减算法相比于分布式梯度下降，显著降低了通信开销并利于保护隐私。在分析中，研究人员引入了 restricted smoothness 衡量本地目标函数和全局目标函数之差的平滑性，结果显示 restricted smoothness 决定了算法的收敛性。并且当数据不平衡，restricted smoothness 较差时，引入差异正则项，保证算法可以收敛。表2列举了各个分布式算法的通信和计算复杂度比较（*标识文中的算法），可以看出文中的分析对最自然的分布式方差缩减算法给出了最优的结果。

<img src="https://pic4.zhimg.com/80/v2-60b5076393228135e4fa86b43bb460f3_720w.jpg">

表 2：分布式算法通信和计算复杂度比较

<b>联邦学习的第二层受限是隐私保护。</b>直觉上，只共享参数更新而不共享原始数据可以在一定程度上保护原始数据的隐私。不过，论文[9]指出在深度模型中共享单个样本的梯度可以泄露原始数据。他们提出梯度匹配（gradient matching）算法，利用给定输入在模型上的梯度可以相当准确地恢复出原始数据的输入和标签。图5展示了梯度匹配在 MNIST、SVHN、CIFAR、LFW 数据集上恢复输入的例子。虽然在复合多个样本点的梯度时，梯度匹配算法会失效，但这篇文章仍然展示了梯度对原始数据的暴露。通常，联邦学习的算法会使用前面介绍的机密计算的工具（同态加密或多方安全计算）来实现可证明的隐私保护[10，11]，只是这些机密计算的方法会带来很大额外的计算和通信开销。

### 23.5.4.2 联邦学习下的模型安全

See through Gradients: Image Batch Recovery via GradInversion

https://arxiv.org/pdf/2104.07586.pdf

🏷️  &nbsp; ``（待补充。。。）``

## 23.6 模型可用性



## 总结



## 小练习 —— 深度学习在计算机视觉中魅力

### 项目实战22 - 神经网络模型嵌入盲水印

<img src="https://github.com/hsouri/Sleeper-Agent/raw/master/schematic.png">

https://github.com/hsouri/Sleeper-Agent

### 项目实战23 - 利用深度学习在图像中嵌入盲水印

<img src="https://github.com/tancik/StegaStamp/raw/master/docs/teaser.png">

https://github.com/Charmve/StegaStamp-plus

<br>

> 本文引用自@左左薇拉，https://www.zuozuovera.com/archives/1637/

## 补充阅读

[1] Xiao H, Biggio B, Brown G, et al. Is feature selection secure against training data poisoning?[C]//International Conference on Machine Learning. 2015: 1689-1698.

[2] Li B, Wang Y, Singh A, et al. Data poisoning attacks on factorization-based collaborative filtering[C]//Advances in neural information processing systems. 2016: 1885-1893.

[3] Alfeld S, Zhu X, Barford P. Data Poisoning Attacks against Autoregressive Models[C]//AAAI. 2016: 1452-1458.

[4] Marsh D O, Myers G J, Clarkson T W, et al. Fetal methylmercury poisoning: clinical and toxicological data on 29 cases[J]. Annals of Neurology: Official Journal of the American Neurological Association and the Child Neurology Society, 1980, 7(4): 348-353.

[5] 陈宇飞, 沈超, 王骞,等. 人工智能系统安全与隐私风险[J]. 计算机研究与发展, 2019, 56(10).

## 参考文献

[1] SzegedyC, Zaremba W, Sutskever I, et al. Intriguing propertiesof neural networks[J]. arXiv preprint arXiv:1312.6199, 2013. 

[2] Chakraborty A, Alam M, Dey V, et al. Adversarial attacks and defences: A survey[J]. arXiv preprint arXiv:1810.00069, 2018.

[3] Zhang J, Gu Z, Jang J, et al. Protecting intellectual property of deep neural networks with watermarking[C]//Proceedings of the 2018 on Asia Conference on Computer and Communications Security. 2018: 159-172.

[4] Tramèr F, Zhang F, Juels A, et al. Stealing machine learning models via prediction apis[C]//25th {USENIX} Security Symposium ({USENIX} Security 16). 2016: 601-618.

[5] Shokri R, Stronati M, Song C, et al. Membership inference attacks against machine learning models[C]//2017 IEEE Symposium on Security and Privacy (SP). IEEE, 2017: 3-18.

[6] Fredrikson M, Lantz E, Jha S, et al. Privacy in Pharmacogenetics: An End-to-End Case Study of Personalized Warfarin Dosing[C]//USENIX Security Symposium. 2014: 17-32.

[7] Fredrikson M, Jha S, Ristenpart T. Model inversion attacks that exploit confidence information and basic countermeasures[C]//Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security. ACM, 2015: 1322-1333.

[8] Cen, Shicong, et al. Convergence of distributed stochastic variance reduced methods without sampling extra data. IEEE Transactions on Signal Processing (2020).

[9] Zhu, Ligeng, et al. Deep Leakage from Gradients, NeurIPS 2019

[10] Hardy, Stephen, et al. Private federated learning on vertically partitioned data via entity resolution and additively homomorphic encryption. arXiv preprint arXiv:1711.10677 (2017).

[11] Yang, Qiang, et al. Federated machine learning: Concept and applications. ACM Transactions on Intelligent Systems and Technology (TIST) 10.2 (2019): 1-19.

[1] Szegedy C, Zaremba W, Sutskever I, et al. Intriguing properties of neural networks[J]. arXiv preprint arXiv:1312.6199, 2013.

[2] Lu J, Sibai H, Fabry E. Adversarial Examples that Fool Detectors[J]. arXiv preprint arXiv:1712.02494, 2017.

[3] Sharif M, Bhagavatula S, Bauer L, et al. Accessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition[C]//Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security. ACM, 2016: 1528-1540.

[4] Carlini N, Mishra P, Vaidya T, et al. Hidden Voice Commands[C]//USENIX Security Symposium. 2016: 513-530.

[5] Goodfellow I J, Shlens J, Szegedy C. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.

[6] Kurakin A, Goodfellow I, Bengio S. Adversarial examples in the physical world. arXiv preprint arXiv:1607.02533, 2016.

[7] <EYD与机器学习>：对抗攻击基础知识（一）. https://zhuanlan.zhihu.com/p/37260275.

[8] https://medium.com/onfido-tech/adversarial-attacks-and-defences-for-convolutional-neural-networks-66915ece52e7

[9] Moosavi-Dezfooli S M, Fawzi A, Frossard P. Deepfool: a simple and accurate method to fool deep neural networks[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016: 2574-2582.

[10] Goodfellow M, Jones A L. Laceyella[J]. Bergey's Manual of Systematics of Archaea and Bacteria, 2015: 1-4.

[11] Papernot N, McDaniel P, Goodfellow I, et al. Practical black-box attacks against machine learning[C]//Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security. ACM, 2017: 506-519.

[12] Papernot N, McDaniel P, Wu X, et al. Distillation as a defense to adversarial
perturbations against deep neural networks[C]//2016 IEEE Symposium on Security and Privacy (SP). IEEE, 2016: 582-597.

[13] Xie C, Wang J, Zhang Z, et al. Mitigating adversarial effects through randomization[J]. arXiv preprint arXiv:1711.01991, 2017.

[14] Liao F, Liang M, Dong Y, et al. Defense against adversarial attacks using high-level representation guided denoiser[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 1778-1787.

